{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "LHTeOZzUQsME"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_math_problem():\n",
        "    problem_type = random.randint(1, 8)\n",
        "\n",
        "    if problem_type == 1:\n",
        "        name = random.choice(['John', 'Sara', 'Alex'])\n",
        "        a = random.randint(1, 10)\n",
        "        b = random.randint(1, 10)\n",
        "        item = random.choice(['apples', 'candies'])\n",
        "        return f\"{name} has {a} {item}. They get {b} more. How many do they have now? Answer: {a + b}.\"\n",
        "\n",
        "    elif problem_type == 2:\n",
        "        v = random.randint(30, 100)\n",
        "        t = random.randint(1, 5)\n",
        "        return f\"A car travels at {v} km/h for {t} hours. How far does it go? Answer: {v * t} km.\"\n",
        "\n",
        "    elif problem_type == 3:\n",
        "        w = random.randint(2, 10)\n",
        "        l = random.randint(2, 10)\n",
        "        return f\"A rectangle has width {w} and length {l}. What is the area? Answer: {w * l}.\"\n",
        "\n",
        "    elif problem_type == 4:\n",
        "        p = random.randint(1000, 5000)\n",
        "        s = random.randint(100, 900)\n",
        "        return f\"A person has ${p} and spends ${s}. How much is left? Answer: ${p - s}.\"\n",
        "\n",
        "    elif problem_type == 5:\n",
        "        r = random.randint(5, 20)\n",
        "        h = random.randint(1, 10)\n",
        "        return f\"A machine produces {r} units/hour. How many in {h} hours? Answer: {r * h} units.\"\n",
        "\n",
        "    elif problem_type == 6:\n",
        "        distance = random.randint(50, 200)\n",
        "        return f\"A train moves {distance} km in 2 hours. What is the speed? Answer: {distance // 2} km/h.\"\n",
        "\n",
        "    elif problem_type == 7:\n",
        "        side1 = random.randint(3, 15)\n",
        "        side2 = random.randint(3, 15)\n",
        "        return f\"Rectangle with sides {side1} and {side2}. Perimeter? Answer: {2 * (side1 + side2)}.\"\n",
        "\n",
        "    else:  # problem_type == 8\n",
        "        total = random.randint(20, 50)\n",
        "        given = random.randint(5, 20)\n",
        "        return f\"You have {total} marbles and give away {given}. How many left? Answer: {total - given}.\"\n",
        "\n",
        "math_data = [generate_math_problem() for _ in range(100)]\n",
        "\n",
        "# Print a few examples to see the output\n",
        "for i in range(5):\n",
        "    print(f\"Problem {i+1}: {math_data[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4snQPKDxTgPn",
        "outputId": "1386537f-1c13-4671-9746-1fcb90d46e82"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 1: You have 46 marbles and give away 12. How many left? Answer: 34.\n",
            "Problem 2: A rectangle has width 5 and length 4. What is the area? Answer: 20.\n",
            "Problem 3: A person has $4663 and spends $453. How much is left? Answer: $4210.\n",
            "Problem 4: A car travels at 80 km/h for 4 hours. How far does it go? Answer: 320 km.\n",
            "Problem 5: A machine produces 19 units/hour. How many in 1 hours? Answer: 19 units.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(toy_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BmbeLgdUqIz",
        "outputId": "1c5b80da-43ab-4a8a-985b-4228439c3a9f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Id2bvQtiQ8yy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Include all characters including newlines\n",
        "all_text = \"\\n\".join(math_data)\n",
        "chars = sorted(list(set(all_text)))  # now includes '\\n'\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Build vocab\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l])\n"
      ],
      "metadata": {
        "id": "by5mLYpvQxzQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSc9u5feQ5Ck",
        "outputId": "a0737bf5-e721-4d1f-dd8f-435ef55683b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpNz-uiPRA1W",
        "outputId": "7b77b241-0e97-4e60-e596-30562ef24225"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "block_size = 64\n",
        "\n",
        "\n",
        "full_text = \"\\n\".join(math_data)\n",
        "data = encode(full_text)\n",
        "\n",
        "\n",
        "n = int(0.9 * len(data))\n",
        "train_data = torch.tensor(data[:n])\n",
        "val_data = torch.tensor(data[n:])\n",
        "\n",
        "def get_batch(split, batch_size=32):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "QgjIje7TU6bz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size, block_size=128, n_layer=4, n_head=4, n_embd=64, dropout=0.1):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config.n_embd,\n",
        "            nhead=config.n_head,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.TransformerEncoder(encoder_layer, num_layers=config.n_layer)\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02)\n",
        "        nn.init.normal_(self.head.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        if T > self.config.block_size:\n",
        "            raise ValueError(f\"Input too long ({T} tokens), max is {self.config.block_size}\")\n",
        "\n",
        "        tok_emb = self.token_embedding(idx)              # (B, T, C)\n",
        "        pos_emb = self.pos_embedding[:, :T, :]           # (1, T, C)\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Generate a causal mask to prevent attending to future tokens\n",
        "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(idx.device)  # (T, T)\n",
        "        x = self.blocks(x, src_key_padding_mask=None, mask=mask)  # causal mask applied here\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)  # (B, T, vocab_size)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "oqGTP2kzRCct"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GPT(GPTConfig(vocab_size)).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(4000):\n",
        "    x, y = get_batch('train')\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    logits = model(x)\n",
        "    B, T, C = logits.shape\n",
        "    loss = F.cross_entropy(logits.view(B*T, C), y.view(B*T))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg7YGrY9RQTL",
        "outputId": "e690f89b-9c7b-4a80-dfc9-9fba58a9bf9b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 3.9098\n",
            "Step 100, Loss: 1.5354\n",
            "Step 200, Loss: 0.5473\n",
            "Step 300, Loss: 0.3424\n",
            "Step 400, Loss: 0.3003\n",
            "Step 500, Loss: 0.2764\n",
            "Step 600, Loss: 0.2538\n",
            "Step 700, Loss: 0.2379\n",
            "Step 800, Loss: 0.2297\n",
            "Step 900, Loss: 0.2412\n",
            "Step 1000, Loss: 0.2367\n",
            "Step 1100, Loss: 0.1890\n",
            "Step 1200, Loss: 0.2008\n",
            "Step 1300, Loss: 0.1823\n",
            "Step 1400, Loss: 0.1797\n",
            "Step 1500, Loss: 0.1688\n",
            "Step 1600, Loss: 0.1611\n",
            "Step 1700, Loss: 0.1633\n",
            "Step 1800, Loss: 0.1505\n",
            "Step 1900, Loss: 0.1552\n",
            "Step 2000, Loss: 0.1423\n",
            "Step 2100, Loss: 0.1290\n",
            "Step 2200, Loss: 0.1397\n",
            "Step 2300, Loss: 0.1406\n",
            "Step 2400, Loss: 0.1324\n",
            "Step 2500, Loss: 0.1319\n",
            "Step 2600, Loss: 0.1301\n",
            "Step 2700, Loss: 0.1162\n",
            "Step 2800, Loss: 0.1187\n",
            "Step 2900, Loss: 0.1278\n",
            "Step 3000, Loss: 0.1196\n",
            "Step 3100, Loss: 0.1222\n",
            "Step 3200, Loss: 0.1284\n",
            "Step 3300, Loss: 0.1270\n",
            "Step 3400, Loss: 0.1239\n",
            "Step 3500, Loss: 0.1031\n",
            "Step 3600, Loss: 0.1047\n",
            "Step 3700, Loss: 0.1201\n",
            "Step 3800, Loss: 0.1135\n",
            "Step 3900, Loss: 0.1225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(idx, max_new_tokens=100):\n",
        "    model.eval()\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, next_token), dim=1)\n",
        "    return idx\n",
        "\n",
        "# Generate\n",
        "start = torch.tensor([[stoi[\"A\"]]]).to(device)\n",
        "out = generate(start, max_new_tokens=200)\n",
        "print(decode(out[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKLwBOJkRTf3",
        "outputId": "37dfecfe-0f46-4188-c944-0e58221e53af"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 14.\n",
            "A person has $2685 and spends $548. How much is left? Answer: $933.\n",
            "Sara has 9 candies. They get 2 more. How many do they have now? Answer: 7.\n",
            "A person has $15000 and spends $548. How much \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(prompt: str, max_new_tokens: int = 100, temperature: float = 1.0, top_k: int = None):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    idx = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        vocab_size = logits.shape[-1]\n",
        "        k = min(top_k if top_k is not None else vocab_size, vocab_size)\n",
        "\n",
        "        if top_k is not None:\n",
        "            top_logits, top_indices = torch.topk(logits, k)\n",
        "            probs = F.softmax(top_logits, dim=-1)\n",
        "            next_token = top_indices.gather(1, torch.multinomial(probs, num_samples=1))\n",
        "        else:\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "    return decode(idx[0].tolist())\n",
        "\n"
      ],
      "metadata": {
        "id": "CHnimoB-SMsQ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = generate(\n",
        "    prompt=\"A train travels\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.8,\n",
        "    top_k=20  # smaller than vocab size\n",
        ")\n",
        "print(output_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc_JsmCSXTww",
        "outputId": "288dd802-f1c8-4e79-fcf9-7b4547944b69"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A train travels at 45 km/h for 2 hours. How far does it go? Answer: 90 km.\n",
            "A car travels at 78 km/h for 1 hours. Ho\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.state_dict(), \"mini_gpt_v2.pth\")\n"
      ],
      "metadata": {
        "id": "hI1FAq-IXiUo"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K5TjYdjIYRWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}